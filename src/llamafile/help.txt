OPTIONS                                                                                                                                                                                    
       The following options are available:                                                                                                                                                
                                                                                                                                                                                           
       --version                                                                                                                                                                           
               Print version and exit.                                                                                                                                                     
                                                                                                                                                                                           
       -h, --help                                                                                                                                                                          
               Show help message and exit.                                                                                                                                                 
                                                                                                                                                                                           
       -m FNAME, --model FNAME                                                                                                                                                             
               Model path in the GGUF file format.                                                                                                                                         
                                                                                                                                                                                           
               Default: models/7B/ggml-model-f16.gguf                                                                                                                                      
                                                                                                                                                                                           
       --mmproj FNAME                                                                                                                                                                      
               Specifies  path of the LLaVA vision model in the GGUF file for‐                                                                                                             
               mat. If this flag is supplied, then  the  --model  and  --image                                                                                                             
               flags should also be supplied.                                                                                                                                              
                                                                                                                                                                                           
       -ngl N, --n-gpu-layers N                                                                                                                                                            
               Enables GPU by specifying number of layers to store in VRAM.                                                                                                                
                                                                                                                                                                                           
               By  default,  llamafile runs in CPU mode. The only exception is                                                                                                             
               Apple Metal, which is reliable enough to be enabled by default.                                                                                                             
               So if you have an NVIDIA or AMD GPU in your  system,  then  you                                                                                                             
               need  to pass this flag to enable GPU support. The simplest way                                                                                                             
               to do this is to say:                                                                                                                                                       
                                                                                                                                                                                           
                     llamafile -ngl 999 -m model.gguf                                                                                                                                      
                                                                                                                                                                                           
               Which will cause llamafile to offload as many layers to the GPU                                                                                                             
               as possible. If you get an out of memory error,  then  you  may                                                                                                             
               tune this to a smaller number, e.g. 10, to ask llamafile to use                                                                                                             
               both CPU and GPU when running your model.                                                                                                                                   
                                                                                                                                                                                           
       --gpu GPU                                                                                                                                                                           
               Specifies which brand of GPU should be used. Valid choices are:                                                                                                             
                                                                                                                                                                                           
               -   AUTO:  Use  any GPU if possible, otherwise fall back to CPU                                                                                                             
                   inference                                                                                                                                                               
                                                                                                                                                                                           
               -   APPLE: Use Apple Metal GPU. This is only available on MacOS                                                                                                             
                   ARM64. If Metal could not be used for any  reason,  then  a                                                                                                             
                   fatal error will be raised.                                                                                                                                             
                                                                                                                                                                                           
               -   AMD: Use AMD GPUs. The AMD HIP ROCm SDK should be installed                                                                                                             
                   in  which  case we assume the HIP_PATH environment variable                                                                                                             
                   has been defined. The set of gfx microarchitectures  needed                                                                                                             
                   to  run  on  the  host  machine is determined automatically                                                                                                             
                   based on the output of the  hipInfo  command.  On  Windows,                                                                                                             
                   llamafile  release binaries are distributed with a tinyBLAS                                                                                                             
                   DLL so it'll work out of the box without requiring the  HIP                                                                                                             
                   SDK  to  be  installed.  However,  tinyBLAS  is slower than                                                                                                             
                   rocBLAS for batch and image processing, so it's recommended                                                                                                             
                   that the SDK be installed anyway. If an AMD GPU  could  not                                                                                                             
                   be used for any reason, then a fatal error will be raised.                                                                                                              
                                                                                                                                                                                           
               -   NVIDIA: Use NVIDIA GPUs. If an NVIDIA GPU could not be used                                                                                                             
                   for  any  reason, a fatal error will be raised. On Windows,                                                                                                             
                   NVIDIA GPU support will use our tinyBLAS library, since  it                                                                                                             
                   works  on  stock  Windows  installs. However, tinyBLAS goes                                                                                                             
                   slower for batch and image processing. It's possible to use                                                                                                             
                   NVIDIA's closed-source cuBLAS library instead. To do  that,                                                                                                             
                   both  MSVC  and CUDA need to be installed and the llamafile                                                                                                             
                   command should be run once from the x64 MSVC command prompt                                                                                                             
                   with the --recompile flag passed.  The  GGML  library  will                                                                                                             
                   then  be compiled and saved to ~/.llamafile/ so the special                                                                                                             
                   process only needs to happen a single time.                                                                                                                             
                                                                                                                                                                                           
               -   DISABLE: Never use GPU and instead use CPU inference.  This                                                                                                             
                   setting is implied by -ngl 0.                                                                                                                                           
                                                                                                                                                                                           
               This  flag  is  useful  on  systems that have multiple kinds of                                                                                                             
               GPUs. For example, if you have two graphics cards in your  com‐                                                                                                             
               puter,  one being AMD and the other is NVIDIA, then you can use                                                                                                             
               this flag to force llamafile to use a particular brand.                                                                                                                     
                                                                                                                                                                                           
               This flag is also useful for preventing CPU fallback. For exam‐                                                                                                             
               ple, if you pass --gpu metal and llamafile is running on  a  PC                                                                                                             
               with  an  NVIDIA card, then the process will print an error and                                                                                                             
               exit.                                                                                                                                                                       
                                                                                                                                                                                           
               The default behavior is AUTO however it should  be  noted  that                                                                                                             
               GPU  support  isn't  enabled by default, since the -ngl flag is                                                                                                             
               normally used to enable GPU offloading. As  a  convenience,  if                                                                                                             
               the  --gpu  flag  is explicitly passed on the command line, and                                                                                                             
               it's set to AUTO, AMD, APPLE, or NVIDIA, but the -ngl  flag  is                                                                                                             
               not passed, then the number of GPU layers will be automatically                                                                                                             
               set to 999.                                                                                                                                                                 
                                                                                                                                                                                           
       -s SEED, --seed SEED                                                                                                                                                                
               Random  Number  Generator  (RNG) seed. A random seed is used if                                                                                                             
               this is less than zero.                                                                                                                                                     
                                                                                                                                                                                           
               Default: -1                                                                                                                                                                 
                                                                                                                                                                                           
       -t N, --threads N                                                                                                                                                                   
               Number of threads to use during generation.                                                                                                                                 
                                                                                                                                                                                           
               Default: $(nproc)/2 max 20                                                                                                                                                  
                                                                                                                                                                                           
       -tb N, --threads-batch N                                                                                                                                                            
               Number of threads to use during prompt processing.                                                                                                                          
                                                                                                                                                                                           
               Default: $(nproc)/2                                                                                                                                                         
                                                                                                                                                                                           
       -c N, --ctx-size N                                                                                                                                                                  
               Sets the maximum context size, in tokens. In --chat mode,  this                                                                                                             
               value  sets  a hard limit on how long your conversation can be.                                                                                                             
               The default is 8192 tokens. If this value is zero,  then  it'll                                                                                                             
               be set to the maximum context size the model allows.                                                                                                                        
                                                                                                                                                                                           
       -b N, --batch-size N                                                                                                                                                                
               Set batch size for prompt processing.                                                                                                                                       
                                                                                                                                                                                           
               Default: 2048                                                                                                                                                               
                                                                                                                                                                                           
       --top-k N                                                                                                                                                                           
               Limits next token selection to K most probable tokens.                                                                                                                      
                                                                                                                                                                                           
               Top-k  sampling  is  a  text generation method that selects the                                                                                                             
               next token only from the top k most likely tokens predicted  by                                                                                                             
               the model. It helps reduce the risk of generating low-probabil‐                                                                                                             
               ity  or nonsensical tokens, but it may also limit the diversity                                                                                                             
               of the output. A higher value for top-k (e.g., 100)  will  con‐                                                                                                             
               sider  more tokens and lead to more diverse text, while a lower                                                                                                             
               value (e.g., 10) will focus on the  most  probable  tokens  and                                                                                                             
               generate more conservative text.                                                                                                                                            
                                                                                                                                                                                           
               Default: 40                                                                                                                                                                 
                                                                                                                                                                                           
       --top-p N                                                                                                                                                                           
               Limits  next token selection to a subset of tokens with a cumu‐                                                                                                             
               lative probability above a threshold P.                                                                                                                                     
                                                                                                                                                                                           
               Top-p sampling, also known as nucleus sampling, is another text                                                                                                             
               generation method that selects the next token from a subset  of                                                                                                             
               tokens  that together have a cumulative probability of at least                                                                                                             
               p. This method provides a balance between diversity and quality                                                                                                             
               by considering both the probabilities of tokens and the  number                                                                                                             
               of tokens to sample from. A higher value for top-p (e.g., 0.95)                                                                                                             
               will lead to more diverse text, while a lower value (e.g., 0.5)                                                                                                             
               will generate more focused and conservative text.                                                                                                                           
                                                                                                                                                                                           
               Default: 0.9                                                                                                                                                                
                                                                                                                                                                                           
       --min-p N                                                                                                                                                                           
               Sets minimum base probability threshold for token selection.                                                                                                                
                                                                                                                                                                                           
               The  Min-P  sampling  method  was designed as an alternative to                                                                                                             
               Top-P, and aims to ensure a balance of quality and variety. The                                                                                                             
               parameter p represents the minimum probability for a  token  to                                                                                                             
               be  considered,  relative to the probability of the most likely                                                                                                             
               token. For example, with p=0.05 and the most likely token  hav‐                                                                                                             
               ing  a  probability of 0.9, logits with a value less than 0.045                                                                                                             
               are filtered out.                                                                                                                                                           
                                                                                                                                                                                           
               Default: 0.05                                                                                                                                                               
                                                                                                                                                                                           
       --tfs N                                                                                                                                                                             
               Enables tail free sampling with parameter z.                                                                                                                                
                                                                                                                                                                                           
               Tail free sampling (TFS) is a text  generation  technique  that                                                                                                             
               aims  to  reduce the impact of less likely tokens, which may be                                                                                                             
               less relevant, less coherent, or nonsensical,  on  the  output.                                                                                                             
               Similar  to  Top-P  it  tries to determine the bulk of the most                                                                                                             
               likely tokens dynamically. But TFS filters out logits based  on                                                                                                             
               the  second derivative of their probabilities. Adding tokens is                                                                                                             
               stopped after the sum of the second derivatives reaches the pa‐                                                                                                             
               rameter z. In short: TFS looks how quickly the probabilities of                                                                                                             
               the tokens decrease and cuts off the tail  of  unlikely  tokens                                                                                                             
               using the parameter z. Typical values for z are in the range of                                                                                                             
               0.9  to 0.95. A value of 1.0 would include all tokens, and thus                                                                                                             
               disables the effect of TFS.                                                                                                                                                 
                                                                                                                                                                                           
               Default: 1.0 (which means disabled)                                                                                                                                         
                                                                                                                                                                                           
       --typical N                                                                                                                                                                         
               Enables locally typical sampling with parameter p.                                                                                                                          
                                                                                                                                                                                           
               Locally typical sampling promotes the generation  of  contextu‐                                                                                                             
               ally coherent and diverse text by sampling tokens that are typ‐                                                                                                             
               ical  or  expected based on the surrounding context. By setting                                                                                                             
               the parameter p between 0 and 1, you can  control  the  balance                                                                                                             
               between  producing text that is locally coherent and diverse. A                                                                                                             
               value closer to 1 will promote more contextually  coherent  to‐                                                                                                             
               kens,  while  a value closer to 0 will promote more diverse to‐                                                                                                             
               kens. A value equal to 1 disables locally typical sampling.                                                                                                                 
                                                                                                                                                                                           
               Default: 1.0 (which means disabled)                                                                                                                                         
                                                                                                                                                                                           
       --repeat-penalty N                                                                                                                                                                  
               Controls repetition of token sequences in generated text.                                                                                                                   
                                                                                                                                                                                           
               This can help prevent the model from generating  repetitive  or                                                                                                             
               monotonous text. A higher value (e.g., 1.5) will penalize repe‐                                                                                                             
               titions  more strongly, while a lower value (e.g., 0.9) will be                                                                                                             
               more lenient.                                                                                                                                                               
                                                                                                                                                                                           
               Default: 1.1                                                                                                                                                                
                                                                                                                                                                                           
       --repeat-last-n N                                                                                                                                                                   
               Last n tokens to consider for penalizing repetition.                                                                                                                        
                                                                                                                                                                                           
               This controls the number of tokens in the history  to  consider                                                                                                             
               for  penalizing  repetition.  A  larger value will look further                                                                                                             
               back in the generated text  to  prevent  repetitions,  while  a                                                                                                             
               smaller  value  will  only consider recent tokens. A value of 0                                                                                                             
               disables the penalty, and a value of -1 sets the number of  to‐                                                                                                             
               kens considered equal to the context size.                                                                                                                                  
                                                                                                                                                                                           
               -   0 = disabled                                                                                                                                                            
               -   -1 = ctx_size                                                                                                                                                           
                                                                                                                                                                                           
               Default: 64                                                                                                                                                                 
                                                                                                                                                                                           
       --no-penalize-nl                                                                                                                                                                    
               Disables  penalization  of newline tokens when applying the re‐                                                                                                             
               peat penalty.                                                                                                                                                               
                                                                                                                                                                                           
               This option is particularly useful for generating chat  conver‐                                                                                                             
               sations, dialogues, code, poetry, or any text where newline to‐                                                                                                             
               kens  play a significant role in structure and formatting. Dis‐                                                                                                             
               abling newline penalization helps maintain the natural flow and                                                                                                             
               intended formatting in these specific use cases.                                                                                                                            
                                                                                                                                                                                           
       --presence-penalty N                                                                                                                                                                
               Repeat alpha presence penalty.                                                                                                                                              
                                                                                                                                                                                           
               -   0.0 = disabled                                                                                                                                                          
                                                                                                                                                                                           
               Default: 0.0                                                                                                                                                                
                                                                                                                                                                                           
       --frequency-penalty N                                                                                                                                                               
               Repeat alpha frequency penalty.                                                                                                                                             
                                                                                                                                                                                           
               -   0.0 = disabled                                                                                                                                                          
                                                                                                                                                                                           
               Default: 0.0                                                                                                                                                                
                                                                                                                                                                                           
       --mirostat N                                                                                                                                                                        
               Use Mirostat sampling.                                                                                                                                                      
                                                                                                                                                                                           
               Mirostat is an algorithm that actively maintains the quality of                                                                                                             
               generated text within a desired range during  text  generation.                                                                                                             
               It  aims  to  strike a balance between coherence and diversity,                                                                                                             
               avoiding low-quality  output  caused  by  excessive  repetition                                                                                                             
               (boredom traps) or incoherence (confusion traps).                                                                                                                           
                                                                                                                                                                                           
               Using Mirostat causes the Top K, Nucleus, Tail Free and Locally                                                                                                             
               Typical samplers parameter to be ignored if used.                                                                                                                           
                                                                                                                                                                                           
               -   0 = disabled                                                                                                                                                            
               -   1 = Mirostat                                                                                                                                                            
               -   2 = Mirostat 2.0                                                                                                                                                        
                                                                                                                                                                                           
               Default: 0                                                                                                                                                                  
                                                                                                                                                                                           
       --mirostat-lr N                                                                                                                                                                     
               Sets the Mirostat learning rate (eta).                                                                                                                                      
                                                                                                                                                                                           
               The learning rate influences how quickly the algorithm responds                                                                                                             
               to feedback from the generated text. A lower learning rate will                                                                                                             
               result in slower adjustments, while a higher learning rate will                                                                                                             
               make the algorithm more responsive.                                                                                                                                         
                                                                                                                                                                                           
               Default: 0.1                                                                                                                                                                
                                                                                                                                                                                           
       --mirostat-ent N                                                                                                                                                                    
               Sets the Mirostat target entropy (tau).                                                                                                                                     
                                                                                                                                                                                           
               This  represents the desired perplexity value for the generated                                                                                                             
               text.  Adjusting the target entropy allows you to  control  the                                                                                                             
               balance  between coherence and diversity in the generated text.                                                                                                             
               A lower value will result in more focused  and  coherent  text,                                                                                                             
               while  a higher value will lead to more diverse and potentially                                                                                                             
               less coherent text.                                                                                                                                                         
                                                                                                                                                                                           
               Default: 5.0                                                                                                                                                                
                                                                                                                                                                                           
       -l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS                                                                                                                                
               Modifies the likelihood of token appearing in  the  completion,                                                                                                             
               i.e.   --logit-bias  15043+1  to  increase  likelihood of token                                                                                                             
               ' Hello', or --logit-bias 15043-1 to decrease likelihood of to‐                                                                                                             
               ken ' Hello'.                                                                                                                                                               
                                                                                                                                                                                           
       --cfg-negative-prompt PROMPT                                                                                                                                                        
               Negative prompt to use for guidance..                                                                                                                                       
                                                                                                                                                                                           
               Default: empty                                                                                                                                                              
                                                                                                                                                                                           
       --cfg-negative-prompt-file FNAME                                                                                                                                                    
               Negative prompt file to use for guidance.                                                                                                                                   
                                                                                                                                                                                           
               Default: empty                                                                                                                                                              
                                                                                                                                                                                           
       --cfg-scale N                                                                                                                                                                       
               Strength of guidance.                                                                                                                                                       
                                                                                                                                                                                           
               -   1.0 = disable                                                                                                                                                           
                                                                                                                                                                                           
               Default: 1.0                                                                                                                                                                
                                                                                                                                                                                           
       --rope-scaling {none,linear,yarn}                                                                                                                                                   
               RoPE frequency scaling method, defaults to linear unless speci‐                                                                                                             
               fied by the model                                                                                                                                                           
                                                                                                                                                                                           
       --rope-scale N                                                                                                                                                                      
               RoPE context scaling factor, expands context by a factor  of  N                                                                                                             
               where  N  is  the  linear scaling factor used by the fine-tuned                                                                                                             
               model. Some fine-tuned models have extended the context  length                                                                                                             
               by scaling RoPE. For example, if the original pre-trained model                                                                                                             
               have  a  context  length (max sequence length) of 4096 (4k) and                                                                                                             
               the fine-tuned model have 32k. That is a scaling factor  of  8,                                                                                                             
               and  should work by setting the above --ctx-size to 32768 (32k)                                                                                                             
               and --rope-scale to 8.                                                                                                                                                      
                                                                                                                                                                                           
       --rope-freq-base N                                                                                                                                                                  
               RoPE base frequency, used by NTK-aware scaling.                                                                                                                             
                                                                                                                                                                                           
               Default: loaded from model                                                                                                                                                  
                                                                                                                                                                                           
       --rope-freq-scale N                                                                                                                                                                 
               RoPE frequency scaling factor, expands context by a  factor  of                                                                                                             
               1/N                                                                                                                                                                         
                                                                                                                                                                                           
       --yarn-orig-ctx N                                                                                                                                                                   
               YaRN: original context size of model.                                                                                                                                       
                                                                                                                                                                                           
               Default: 0 = model training context size                                                                                                                                    
                                                                                                                                                                                           
       --yarn-ext-factor N                                                                                                                                                                 
               YaRN: extrapolation mix factor.                                                                                                                                             
                                                                                                                                                                                           
               -   0.0 = full interpolation                                                                                                                                                
                                                                                                                                                                                           
               Default: 1.0                                                                                                                                                                
                                                                                                                                                                                           
       --yarn-attn-factor N                                                                                                                                                                
               YaRN: scale sqrt(t) or attention magnitude.                                                                                                                                 
                                                                                                                                                                                           
               Default: 1.0                                                                                                                                                                
                                                                                                                                                                                           
       --yarn-beta-slow N                                                                                                                                                                  
               YaRN: high correction dim or alpha.                                                                                                                                         
                                                                                                                                                                                           
               Default: 1.0                                                                                                                                                                
                                                                                                                                                                                           
       --yarn-beta-fast N                                                                                                                                                                  
               YaRN: low correction dim or beta.                                                                                                                                           
                                                                                                                                                                                           
               Default: 32.0                                                                                                                                                               
                                                                                                                                                                                           
       --temp N                                                                                                                                                                            
               Adjust the randomness of the generated text.                                                                                                                                
                                                                                                                                                                                           
               Temperature is a hyperparameter that controls the randomness of                                                                                                             
               the  generated text. It affects the probability distribution of                                                                                                             
               the model's output tokens. A  higher  temperature  (e.g.,  1.5)                                                                                                             
               makes  the  output more random and creative, while a lower tem‐                                                                                                             
               perature (e.g., 0.5) makes the output more focused, determinis‐                                                                                                             
               tic, and conservative. The default value is 0.8, which provides                                                                                                             
               a balance between randomness and determinism. At the extreme, a                                                                                                             
               temperature of 0 will always pick the most likely  next  token,                                                                                                             
               leading to identical outputs in each run.                                                                                                                                   
                                                                                                                                                                                           
               Default: 0.8 in cli and server mode, and 0.0 in chat mode                                                                                                                   
                                                                                                                                                                                           
       --logits-all                                                                                                                                                                        
               Return logits for all tokens in the batch.                                                                                                                                  
                                                                                                                                                                                           
               Default: disabled                                                                                                                                                           
                                                                                                                                                                                           
       -ns N, --sequences N                                                                                                                                                                
               Number of sequences to decode.                                                                                                                                              
                                                                                                                                                                                           
               Default: 1                                                                                                                                                                  
                                                                                                                                                                                           
       -pa N, --p-accept N                                                                                                                                                                 
               speculative decoding accept probability.                                                                                                                                    
                                                                                                                                                                                           
               Default: 0.5                                                                                                                                                                
                                                                                                                                                                                           
       -ps N, --p-split N                                                                                                                                                                  
               Speculative decoding split probability.                                                                                                                                     
                                                                                                                                                                                           
               Default: 0.1                                                                                                                                                                
                                                                                                                                                                                           
       --mlock                                                                                                                                                                             
               Force  system to keep model in RAM rather than swapping or com‐                                                                                                             
               pressing.                                                                                                                                                                   
                                                                                                                                                                                           
       --no-mmap                                                                                                                                                                           
               Do not memory-map model (slower load but may reduce pageouts if                                                                                                             
               not using mlock).                                                                                                                                                           
                                                                                                                                                                                           
       --numa  Attempt optimizations that help on some  NUMA  systems  if  run                                                                                                             
               without  this  previously, it is recommended to drop the system                                                                                                             
               page       cache       before       using       this.       See                                                                                                             
               https://github.com/ggerganov/llama.cpp/issues/1437.                                                                                                                         
                                                                                                                                                                                           
       --recompile                                                                                                                                                                         
               Force GPU support to be recompiled at runtime if possible.                                                                                                                  
                                                                                                                                                                                           
       --nocompile                                                                                                                                                                         
               Never compile GPU support at runtime.                                                                                                                                       
                                                                                                                                                                                           
               If  the appropriate DSO file already exists under ~/.llamafile/                                                                                                             
               then it'll be linked as-is without question. If a prebuilt  DSO                                                                                                             
               is  present  in the PKZIP content of the executable, then it'll                                                                                                             
               be extracted and linked if possible. Otherwise, llamafile  will                                                                                                             
               skip any attempt to compile GPU support and simply fall back to                                                                                                             
               using CPU inference.                                                                                                                                                        
                                                                                                                                                                                           
       -sm SPLIT_MODE, --split-mode SPLIT_MODE                                                                                                                                             
               How to split the model across multiple GPUs, one of:                                                                                                                        
               -   none: use one GPU only                                                                                                                                                  
               -   layer (default): split layers and KV across GPUs                                                                                                                        
               -   row: split rows across GPUs                                                                                                                                             
                                                                                                                                                                                           
       -ts SPLIT, --tensor-split SPLIT                                                                                                                                                     
               When using multiple GPUs this option controls how large tensors                                                                                                             
               should  be  split  across all GPUs.  SPLIT is a comma-separated                                                                                                             
               list of non-negative values that assigns the proportion of data                                                                                                             
               that each GPU should get in order. For example, "3,2" will  as‐                                                                                                             
               sign  60% of the data to GPU 0 and 40% to GPU 1. By default the                                                                                                             
               data is split in proportion to VRAM but this may not be optimal                                                                                                             
               for performance. Requires cuBLAS.  How to split tensors  across                                                                                                             
               multiple GPUs, comma-separated list of proportions, e.g. 3,1                                                                                                                
                                                                                                                                                                                           
       -mg i, --main-gpu i                                                                                                                                                                 
               The GPU to use for scratch and small tensors.                                                                                                                               
                                                                                                                                                                                           
       --verbose-prompt                                                                                                                                                                    
               Print prompt before generation.                                                                                                                                             
                                                                                                                                                                                           
       --lora FNAME                                                                                                                                                                        
               Apply LoRA adapter (implies --no-mmap)                                                                                                                                      
                                                                                                                                                                                           
       --lora-scaled FNAME S                                                                                                                                                               
               Apply  LoRA  adapter  with  user  defined  scaling  S  (implies                                                                                                             
               --no-mmap)                                                                                                                                                                  
                                                                                                                                                                                           
       --lora-base FNAME                                                                                                                                                                   
               Optional model to use as a base for the layers modified by  the                                                                                                             
               LoRA adapter                                                                                                                                                                
                                                                                                                                                                                           
       --unsecure                                                                                                                                                                          
               Disables pledge() sandboxing on Linux and OpenBSD.                                                                                                                          
                                                                                                                                                                                           
       --samplers                                                                                                                                                                          
               Samplers  that  will be used for generation in the order, sepa‐                                                                                                             
               rated    by    semicolon,    for    example:    top_k;tfs;typi‐                                                                                                             
               cal;top_p;min_p;temp                                                                                                                                                        
                                                                                                                                                                                           
       --samplers-seq                                                                                                                                                                      
               Simplified sequence for samplers that will be used.                                                                                                                         
                                                                                                                                                                                           
       -dkvc, --dump-kv-cache                                                                                                                                                              
               Verbose print of the KV cache.                                                                                                                                              
                                                                                                                                                                                           
       -nkvo, --no-kv-offload                                                                                                                                                              
               Disable KV offload.                                                                                                                                                         
                                                                                                                                                                                           
       -gan N, --grp-attn-n N                                                                                                                                                              
               Group-attention factor.                                                                                                                                                     
                                                                                                                                                                                           
               Default: 1                                                                                                                                                                  
                                                                                                                                                                                           
       -gaw N, --grp-attn-w N                                                                                                                                                              
               Group-attention width.                                                                                                                                                      
                                                                                                                                                                                           
               Default: 512                                                                                                                                                                
                                                                                                                                                                                           
       -bf FNAME, --binary-file FNAME                                                                                                                                                      
               Binary file containing multiple choice tasks.                                                                                                                               
                                                                                                                                                                                           
       --multiple-choice                                                                                                                                                                   
               Compute  multiple  choice score over random tasks from datafile                                                                                                             
               supplied by the -f flag.                                                                                                                                                    
                                                                                                                                                                                           
       --multiple-choice-tasks N                                                                                                                                                           
               Number of tasks to  use  when  computing  the  multiple  choice                                                                                                             
               score.                                                                                                                                                                      
                                                                                                                                                                                           
               Default: 0                                                                                                                                                                  
                                                                                                                                                                                           
       --kl-divergence                                                                                                                                                                     
               Computes    KL-divergence    to   logits   provided   via   the                                                                                                             
               --kl-divergence-base flag.                                                                                                                                                  
                                                                                                                                                                                           
       --save-all-logits FNAME, --kl-divergence-base FNAME                                                                                                                                 
               Save logits to filename.                                                                                                                                                    
                                                                                                                                                                                           
       -ptc N, --print-token-count N                                                                                                                                                       
               Print token count every N tokens.                                                                                                                                           
                                                                                                                                                                                           
               Default: -1                                                                                                                                                                 
                                                                                                                                                                                           
       --pooling KIND                                                                                                                                                                      
               Specifies pooling type for embeddings. This may be one of:                                                                                                                  
                                                                                                                                                                                           
               -   none                                                                                                                                                                    
               -   mean                                                                                                                                                                    
               -   cls                                                                                                                                                                     
                                                                                                                                                                                           
               The model default is used if unspecified.                                                                                                                                   
                                                                                                                                                                                           
CHAT OPTIONS                                                                                                                                                                               
       The following options may be specified when  llamafile  is  running  in                                                                                                             
       --chat mode.                                                                                                                                                                        
                                                                                                                                                                                           
       -p STRING, --prompt STRING                                                                                                                                                          
               Specifies system prompt.                                                                                                                                                    
                                                                                                                                                                                           
               The  system  prompt  is used to give instructions to the LLM at                                                                                                             
               the beginning of the conversation.  For  many  model  architec‐                                                                                                             
               tures,  this  is  done  under a special role. The system prompt                                                                                                             
               also gets special treatment when managing the  context  window.                                                                                                             
               For  example,  the  /clear command will erase everything except                                                                                                             
               the system prompt, and the /forget command will erase the  old‐                                                                                                             
               est chat message that isn't the system prompt.                                                                                                                              
                                                                                                                                                                                           
               For example:                                                                                                                                                                
                                                                                                                                                                                           
                     llamafile  --chat  -m  model.gguf  -p  "You  are Mosaic's                                                                                                             
                     Godzilla."                                                                                                                                                            
                                                                                                                                                                                           
               may be used to instruct your llamafile to roleplay as Mozilla.                                                                                                              
                                                                                                                                                                                           
       -f FNAME, --file FNAME                                                                                                                                                              
               Uses content of file as system prompt.                                                                                                                                      
                                                                                                                                                                                           
       --no-display-prompt, --silent-prompt                                                                                                                                                
               Suppress printing of system prompt at  beginning  of  conversa‐                                                                                                             
               tion.                                                                                                                                                                       
                                                                                                                                                                                           
       --nologo                                                                                                                                                                            
               Disables printing the llamafile logo during chatbot startup.                                                                                                                
                                                                                                                                                                                           
       --ascii                                                                                                                                                                             
               This  flag  may  be  used in --chat mode to print the llamafile                                                                                                             
               logo in ASCII rather than UNICODE.                                                                                                                                          
                                                                                                                                                                                           
       --verbose                                                                                                                                                                           
               Enables verbose logger output in chatbot. This can  be  helpful                                                                                                             
               for troubleshooting issues.                                                                                                                                                 
                                                                                                                                                                                           
       --chat-template NAME                                                                                                                                                                
               Specifies or overrides chat template for model.                                                                                                                             
                                                                                                                                                                                           
               Normally the GGUF metadata tokenizer.chat_template will specify                                                                                                             
               this value for instruct models. This flag may be used to either                                                                                                             
               override  the chat template, or specify one when the GGUF meta‐                                                                                                             
               data field is absent, which effectively forces the  web  ui  to                                                                                                             
               enable chatbot mode.                                                                                                                                                        
                                                                                                                                                                                           
               Supported chat template names are: chatml, llama2, llama3, mis‐                                                                                                             
               tral  (alias  for llama2), phi3, zephyr, monarch, gemma, gemma2                                                                                                             
               (alias  for  gemma),  orion,  openchat,  vicuna,   vicuna-orca,                                                                                                             
               deepseek, command-r, chatglm3, chatglm4, minicpm, deepseek2, or                                                                                                             
               exaone3.                                                                                                                                                                    
                                                                                                                                                                                           
               It  is also possible to pass the jinja2 template itself to this                                                                                                             
               argument.  Since llamafiler doesn't currently support jinja2, a                                                                                                             
               heuristic will be used to guess which of  the  above  templates                                                                                                             
               the template represents.                                                                                                                                                    
                                                                                                                                                                                           
CLI OPTIONS                                                                                                                                                                                
       The  following  options  may  be specified when llamafile is running in                                                                                                             
       --cli mode.                                                                                                                                                                         
                                                                                                                                                                                           
       -p STRING, --prompt STRING                                                                                                                                                          
               Prompt to start text generation. Your LLM  works  by  auto-com‐                                                                                                             
               pleting this text. For example:                                                                                                                                             
                                                                                                                                                                                           
                     llamafile -m model.gguf -p "four score and"                                                                                                                           
                                                                                                                                                                                           
               Stands  a  pretty  good chance of printing Lincoln's Gettysburg                                                                                                             
               Address.  Prompts can take on a structured format too.  Depend‐                                                                                                             
               ing  on  how your model was trained, it may specify in its docs                                                                                                             
               an instruction notation. With some models that might be:                                                                                                                    
                                                                                                                                                                                           
                     llamafile -p "[INST]Summarize this: $(cat file)[/INST]"                                                                                                               
                                                                                                                                                                                           
               In most cases, simply colons and newlines will work too:                                                                                                                    
                                                                                                                                                                                           
                     llamafile -e -p "User: What is best in life?\nAssistant:"                                                                                                             
                                                                                                                                                                                           
       -f FNAME, --file FNAME                                                                                                                                                              
               Prompt file to start generation.                                                                                                                                            
                                                                                                                                                                                           
       -n N, --n-predict N                                                                                                                                                                 
               Sets number of tokens to predict when generating text.                                                                                                                      
                                                                                                                                                                                           
               This option controls the number of tokens the  model  generates                                                                                                             
               in  response  to the input prompt. By adjusting this value, you                                                                                                             
               can influence the length of the generated text. A higher  value                                                                                                             
               will  result  in  longer text, while a lower value will produce                                                                                                             
               shorter text.                                                                                                                                                               
                                                                                                                                                                                           
               A value of -1 will enable infinite text generation, even though                                                                                                             
               we have a finite context window. When  the  context  window  is                                                                                                             
               full,  some  of  the  earlier  tokens (half of the tokens after                                                                                                             
               --n-keep) will be discarded. The context must then be re-evalu‐                                                                                                             
               ated before generation can resume. On large models and/or large                                                                                                             
               context windows, this will result in significant pause in  out‐                                                                                                             
               put.                                                                                                                                                                        
                                                                                                                                                                                           
               If the pause is undesirable, a value of -2 will stop generation                                                                                                             
               immediately when the context is filled.                                                                                                                                     
                                                                                                                                                                                           
               It  is important to note that the generated text may be shorter                                                                                                             
               than the specified number of tokens if an End-of-Sequence (EOS)                                                                                                             
               token or a reverse prompt is encountered. In  interactive  mode                                                                                                             
               text  generation will pause and control will be returned to the                                                                                                             
               user. In non-interactive mode, the program will  end.  In  both                                                                                                             
               cases,  the text generation may stop before reaching the speci‐                                                                                                             
               fied `n-predict` value. If you want the  model  to  keep  going                                                                                                             
               without  ever producing End-of-Sequence on its own, you can use                                                                                                             
               the --ignore-eos parameter.                                                                                                                                                 
                                                                                                                                                                                           
               -   -1 = infinity                                                                                                                                                           
               -   -2 = until context filled                                                                                                                                               
                                                                                                                                                                                           
               Default: -1                                                                                                                                                                 
                                                                                                                                                                                           
       --simple-io                                                                                                                                                                         
               Use basic IO for better compatibility in subprocesses and  lim‐                                                                                                             
               ited consoles.                                                                                                                                                              
                                                                                                                                                                                           
       -cml, --chatml                                                                                                                                                                      
               Run in chatml mode (use with ChatML-compatible models)                                                                                                                      
                                                                                                                                                                                           
       -e, --escape                                                                                                                                                                        
               Process prompt escapes sequences (\n, \r, \t, \´, \", \\)                                                                                                                   
                                                                                                                                                                                           
       --grammar GRAMMAR                                                                                                                                                                   
               BNF-like grammar to constrain which tokens may be selected when                                                                                                             
               generating text. For example, the grammar:                                                                                                                                  
                                                                                                                                                                                           
                     root ::= "yes" | "no"                                                                                                                                                 
                                                                                                                                                                                           
               will  force  the  LLM  to only output yes or no before exiting.                                                                                                             
               This is useful for shell scripts when  the  --no-display-prompt                                                                                                             
               flag is also supplied.                                                                                                                                                      
                                                                                                                                                                                           
       --grammar-file FNAME                                                                                                                                                                
               File to read grammar from.                                                                                                                                                  
                                                                                                                                                                                           
       --fast  Put  llamafile  into  fast  math mode. This disables algorithms                                                                                                             
               that reduce floating point rounding, e.g. Kahan summation,  and                                                                                                             
               certain functions like expf() will be vectorized but handle un‐                                                                                                             
               derflows  less  gracefully.  It's unspecified whether llamafile                                                                                                             
               runs in fast or precise math mode when neither flag  is  speci‐                                                                                                             
               fied.                                                                                                                                                                       
                                                                                                                                                                                           
       --precise                                                                                                                                                                           
               Put  llamafile  into precise math mode. This enables algorithms                                                                                                             
               that reduce floating point rounding, e.g. Kahan summation,  and                                                                                                             
               certain  functions  like  expf()  will always handle subnormals                                                                                                             
               correctly. It's unspecified whether llamafile runs in  fast  or                                                                                                             
               precise math mode when neither flag is specified.                                                                                                                           
                                                                                                                                                                                           
       --trap  Put  llamafile into math trapping mode. When floating point ex‐                                                                                                             
               ceptions occur, such as NaNs, overflow,  and  divide  by  zero,                                                                                                             
               llamafile  will  print  a  warning to the console. This warning                                                                                                             
               will include a C++ backtrace the first  time  an  exception  is                                                                                                             
               trapped.  The  op graph will also be dumped to a file, and lla‐                                                                                                             
               mafile will report the specific  op  where  the  exception  oc‐                                                                                                             
               curred.  This  is useful for troubleshooting when reporting is‐                                                                                                             
               sues.  USing this feature will disable sandboxing.  Math  trap‐                                                                                                             
               ping  is  only possible if your CPU supports it. That is gener‐                                                                                                             
               ally the case on AMD64, however it's less common on ARM64.                                                                                                                  
                                                                                                                                                                                           
       --prompt-cache FNAME                                                                                                                                                                
               File to cache prompt state for faster startup.                                                                                                                              
                                                                                                                                                                                           
               Default: none                                                                                                                                                               
                                                                                                                                                                                           
       -fa FNAME, --flash-attn                                                                                                                                                             
               Enable Flash Attention. This is a  mathematical  shortcut  that                                                                                                             
               can  speed  up  inference  for  certain models. This feature is                                                                                                             
               still under active development.                                                                                                                                             
                                                                                                                                                                                           
       --prompt-cache-all                                                                                                                                                                  
               If specified, saves user input  and  generations  to  cache  as                                                                                                             
               well. Not supported with --interactive or other interactive op‐                                                                                                             
               tions.                                                                                                                                                                      
                                                                                                                                                                                           
       --prompt-cache-ro                                                                                                                                                                   
               If specified, uses the prompt cache but does not update it.                                                                                                                 
                                                                                                                                                                                           
       --random-prompt                                                                                                                                                                     
               Start with a randomized prompt.                                                                                                                                             
                                                                                                                                                                                           
       --image IMAGE_FILE                                                                                                                                                                  
               Path to an image file. This should be used with multimodal mod‐                                                                                                             
               els.   Alternatively,  it's possible to embed an image directly                                                                                                             
               into the prompt instead; in which case, it must be  base64  en‐                                                                                                             
               coded  into  an HTML img tag URL with the image/jpeg MIME type.                                                                                                             
               See also the --mmproj flag for supplying the vision model.                                                                                                                  
                                                                                                                                                                                           
       -i, --interactive                                                                                                                                                                   
               Run the program in interactive mode, allowing users  to  engage                                                                                                             
               in  real-time conversations or provide specific instructions to                                                                                                             
               the model.                                                                                                                                                                  
                                                                                                                                                                                           
       --interactive-first                                                                                                                                                                 
               Run the program in interactive mode and  immediately  wait  for                                                                                                             
               user input before starting the text generation.                                                                                                                             
                                                                                                                                                                                           
       -ins, --instruct                                                                                                                                                                    
               Run  the program in instruction mode, which is specifically de‐                                                                                                             
               signed to work with Alpaca  models  that  excel  in  completing                                                                                                             
               tasks based on user instructions.                                                                                                                                           
                                                                                                                                                                                           
               Technical details: The user's input is internally prefixed with                                                                                                             
               the  reverse prompt (or "### Instruction:" as the default), and                                                                                                             
               followed by "### Response:" (except if you  just  press  Return                                                                                                             
               without any input, to keep generating a longer response).                                                                                                                   
                                                                                                                                                                                           
               By  understanding  and utilizing these interaction options, you                                                                                                             
               can create engaging and dynamic experiences with the LLaMA mod‐                                                                                                             
               els, tailoring the text generation  process  to  your  specific                                                                                                             
               needs.                                                                                                                                                                      
                                                                                                                                                                                           
       -r PROMPT, --reverse-prompt PROMPT                                                                                                                                                  
               Specify  one  or multiple reverse prompts to pause text genera‐                                                                                                             
               tion and switch to interactive mode. For  example,  -r  "User:"                                                                                                             
               can  be  used  to jump back into the conversation whenever it's                                                                                                             
               the user's turn to speak. This helps create a more  interactive                                                                                                             
               and  conversational  experience.  However,  the  reverse prompt                                                                                                             
               doesn't work when it ends with a space. To overcome this  limi‐                                                                                                             
               tation,  you can use the --in-prefix flag to add a space or any                                                                                                             
               other characters after the reverse prompt.                                                                                                                                  
                                                                                                                                                                                           
       --color                                                                                                                                                                             
               Enable colorized output to differentiate visually  distinguish‐                                                                                                             
               ing between prompts, user input, and generated text.                                                                                                                        
                                                                                                                                                                                           
       --no-display-prompt, --silent-prompt                                                                                                                                                
               Don't echo the prompt itself to standard output.                                                                                                                            
                                                                                                                                                                                           
       --keep N                                                                                                                                                                            
               Specifies number of tokens to keep from the initial prompt. The                                                                                                             
               default is -1 which means all tokens.                                                                                                                                       
                                                                                                                                                                                           
       --multiline-input                                                                                                                                                                   
               Allows you to write or paste multiple lines without ending each                                                                                                             
               in '\'.                                                                                                                                                                     
                                                                                                                                                                                           
       --cont-batching                                                                                                                                                                     
               Enables  continuous  batching,  a.k.a. dynamic batching.  is -1                                                                                                             
               which means all tokens.                                                                                                                                                     
                                                                                                                                                                                           
       --embedding                                                                                                                                                                         
               In CLI mode, the embedding flag may be use to print  embeddings                                                                                                             
               to  standard output. By default, embeddings are computed over a                                                                                                             
               whole prompt. However the --multiline flag may  be  passed,  to                                                                                                             
               have a separate embeddings array computed for each line of text                                                                                                             
               in  the prompt. In multiline mode, each embedding array will be                                                                                                             
               printed on its own line to standard  output,  where  individual                                                                                                             
               floats  are  separated  by space. If both the --multiline-input                                                                                                             
               and --interactive flags are passed, then a pretty-printed  sum‐                                                                                                             
               mary  of  embeddings along with a cosine similarity matrix will                                                                                                             
               be printed to the terminal.                                                                                                                                                 
                                                                                                                                                                                           
       --ignore-eos                                                                                                                                                                        
               Ignore end of stream token  and  continue  generating  (implies                                                                                                             
               --logit-bias 2-inf)                                                                                                                                                         
                                                                                                                                                                                           
       --keep N                                                                                                                                                                            
               This  flag  allows users to retain the original prompt when the                                                                                                             
               model runs out of context, ensuring a connection to the initial                                                                                                             
               instruction or conversation topic is maintained, where N is the                                                                                                             
               number of tokens from the initial prompt  to  retain  when  the                                                                                                             
               model resets its internal context.                                                                                                                                          
                                                                                                                                                                                           
               -   0 = no tokens are kept from initial prompt                                                                                                                              
               -   -1 = retain all tokens from initial prompt                                                                                                                              
                                                                                                                                                                                           
               Default: 0                                                                                                                                                                  
                                                                                                                                                                                           
       --in-prefix-bos                                                                                                                                                                     
               Prefix BOS to user inputs, preceding the --in-prefix string.                                                                                                                
                                                                                                                                                                                           
       --in-prefix STRING                                                                                                                                                                  
               This  flag  is  used  to add a prefix to your input, primarily,                                                                                                             
               this is used to insert a space after the reverse prompt. Here's                                                                                                             
               an example of how to use the --in-prefix  flag  in  conjunction                                                                                                             
               with the --reverse-prompt flag:                                                                                                                                             
                                                                                                                                                                                           
                     ./main -r "User:" --in-prefix " "                                                                                                                                     
                                                                                                                                                                                           
               Default: empty                                                                                                                                                              
                                                                                                                                                                                           
       --in-suffix STRING                                                                                                                                                                  
               This  flag  is  used  to add a suffix after your input. This is                                                                                                             
               useful for adding an "Assistant:" prompt after the  user's  in‐                                                                                                             
               put.  It's added after the new-line character (\n) that's auto‐                                                                                                             
               matically added to the end of the user's input. Here's an exam‐                                                                                                             
               ple of how to use the --in-suffix flag in conjunction with  the                                                                                                             
               --reverse-prompt flag:                                                                                                                                                      
                                                                                                                                                                                           
                     ./main   -r   "User:"   --in-prefix   "   "   --in-suffix                                                                                                             
                     "Assistant:"                                                                                                                                                          
                                                                                                                                                                                           
               Default: empty                                                                                                                                                              
                                                                                                                                                                                           
SERVER OPTIONS                                                                                                                                                                             
       The following options may be specified when  llamafile  is  running  in                                                                                                             
       --server mode.                                                                                                                                                                      
                                                                                                                                                                                           
       --port PORT                                                                                                                                                                         
               Port to listen                                                                                                                                                              
                                                                                                                                                                                           
               Default: 8080                                                                                                                                                               
                                                                                                                                                                                           
       --host IPADDR                                                                                                                                                                       
               IP address to listen.                                                                                                                                                       
                                                                                                                                                                                           
               Default: 127.0.0.1                                                                                                                                                          
                                                                                                                                                                                           
       -to N, --timeout N                                                                                                                                                                  
               Server read/write timeout in seconds.                                                                                                                                       
                                                                                                                                                                                           
               Default: 600                                                                                                                                                                
                                                                                                                                                                                           
       -np N, --parallel N                                                                                                                                                                 
               Number of slots for process requests.                                                                                                                                       
                                                                                                                                                                                           
               Default: 1                                                                                                                                                                  
                                                                                                                                                                                           
       -cb, --cont-batching                                                                                                                                                                
               Enable continuous batching (a.k.a dynamic batching).                                                                                                                        
                                                                                                                                                                                           
               Default: disabled                                                                                                                                                           
                                                                                                                                                                                           
       -spf FNAME, --system-prompt-file FNAME                                                                                                                                              
               Set  a  file  to  load  a  system prompt (initial prompt of all                                                                                                             
               slots), this is useful for chat applications.                                                                                                                               
                                                                                                                                                                                           
       -a ALIAS, --alias ALIAS                                                                                                                                                             
               Set an alias for the model. This will be  added  as  the  model                                                                                                             
               field in completion responses.                                                                                                                                              
                                                                                                                                                                                           
       --path PUBLIC_PATH                                                                                                                                                                  
               Path from which to serve static files.                                                                                                                                      
                                                                                                                                                                                           
               Default: /zip/llama.cpp/server/public                                                                                                                                       
                                                                                                                                                                                           
       --url-prefix PREFIX                                                                                                                                                                 
               Specify a URL prefix (subdirectory) under which the API will be                                                                                                             
               served, e.g. /llamafile                                                                                                                                                     
                                                                                                                                                                                           
               Default: /                                                                                                                                                                  
                                                                                                                                                                                           
       --nobrowser                                                                                                                                                                         
               Do not attempt to open a web browser tab at startup.                                                                                                                        
                                                                                                                                                                                           
       -gan N, --grp-attn-n N                                                                                                                                                              
               Set  the  group attention factor to extend context size through                                                                                                             
               self-extend. The default value is 1 which means disabled.  This                                                                                                             
               flag is used together with --grp-attn-w.                                                                                                                                    
                                                                                                                                                                                           
       -gaw N, --grp-attn-w N                                                                                                                                                              
               Set  the  group  attention width to extend context size through                                                                                                             
               self-extend. The default value is 512.  This flag is  used  to‐                                                                                                             
               gether with --grp-attn-n.                                                                                                                                                   
                                                                                                                                                                                           
LOG OPTIONS                                                                                                                                                                                
       The following log options are available:                                                                                                                                            
                                                                                                                                                                                           
       -ld LOGDIR, --logdir LOGDIR                                                                                                                                                         
               Path under which to save YAML logs (no logging if unset)                                                                                                                    
                                                                                                                                                                                           
       --log-test                                                                                                                                                                          
               Run simple logging test                                                                                                                                                     
                                                                                                                                                                                           
       --log-disable                                                                                                                                                                       
               Disable trace logs                                                                                                                                                          
                                                                                                                                                                                           
       --log-enable                                                                                                                                                                        
               Enable trace logs                                                                                                                                                           
                                                                                                                                                                                           
       --log-file                                                                                                                                                                          
               Specify a log filename (without extension)                                                                                                                                  
                                                                                                                                                                                           
       --log-new                                                                                                                                                                           
               Create  a  separate  new  log file on start. Each log file will                                                                                                             
               have unique name: <name>.<ID>.log                                                                                                                                           
                                                                                                                                                                                           
       --log-append                                                                                                                                                                        
               Don't truncate the old log file.                                                                                                                                            
                                                                                                                                                                                           
EXAMPLES                                                                                                                                                                                   
       Here's an example of how to run llama.cpp's built-in HTTP server.  This                                                                                                             
       example   uses   LLaVA  v1.5-7B,  a  multimodal  LLM  that  works  with                                                                                                             
       llama.cpp's recently-added support for image inputs.                                                                                                                                
                                                                                                                                                                                           
             llamafile \                                                                                                                                                                   
               -m llava-v1.5-7b-Q8_0.gguf \                                                                                                                                                
               --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \                                                                                                                                   
               --host 0.0.0.0                                                                                                                                                              
                                                                                                                                                                                           
       Here's an example of how to generate code for a libc function using the                                                                                                             
       llama.cpp  command  line  interface,  utilizing  WizardCoder-Python-13B                                                                                                             
       weights:                                                                                                                                                                            
                                                                                                                                                                                           
             llamafile \                                                                                                                                                                   
               -m wizardcoder-python-13b-v1.0.Q8_0.gguf --temp 0 -r '}\n' -r '```\n' \                                                                                                     
               -e -p '```c\nvoid *memcpy(void *dst, const void *src, size_t size) {\n'                                                                                                     
                                                                                                                                                                                           
       Here's  a  similar  example  that  instead utilizes Mistral-7B-Instruct                                                                                                             
       weights for prose composition:                                                                                                                                                      
                                                                                                                                                                                           
             llamafile \                                                                                                                                                                   
               -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \                                                                                                                                   
               -p '[INST]Write a story about llamas[/INST]'                                                                                                                                
                                                                                                                                                                                           
       Here's an example of how llamafile can be used as an interactive  chat‐                                                                                                             
       bot that lets you query knowledge contained in training data:                                                                                                                       
                                                                                                                                                                                           
             llamafile -m llama-65b-Q5_K.gguf -p '                                                                                                                                         
             The following is a conversation between a Researcher and their helpful AI                                                                                                     
             assistant Digital Athena which is a large language model trained on the                                                                                                       
             sum of human knowledge.                                                                                                                                                       
             Researcher: Good morning.                                                                                                                                                     
             Digital Athena: How can I help you today?                                                                                                                                     
             Researcher:' --interactive --color --batch_size 1024 --ctx_size 4096 \                                                                                                        
             --keep -1 --temp 0 --mirostat 2 --in-prefix ' ' --interactive-first \                                                                                                         
             --in-suffix 'Digital Athena:' --reverse-prompt 'Researcher:'                                                                                                                  
                                                                                                                                                                                           
       Here's an example of how you can use llamafile to summarize HTML URLs:                                                                                                              
                                                                                                                                                                                           
             (                                                                                                                                                                             
               echo '[INST]Summarize the following text:'                                                                                                                                  
               links -codepage utf-8 \                                                                                                                                                     
                     -force-html \                                                                                                                                                         
                     -width 500 \                                                                                                                                                          
                     -dump https://www.poetryfoundation.org/poems/48860/the-raven |                                                                                                        
                 sed 's/   */ /g'                                                                                                                                                          
               echo '[/INST]'                                                                                                                                                              
             ) | llamafile \                                                                                                                                                               
                   -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \                                                                                                                               
                   -f /dev/stdin \                                                                                                                                                         
                   -c 0 \                                                                                                                                                                  
                   --temp 0 \                                                                                                                                                              
                   -n 500 \                                                                                                                                                                
                   --no-display-prompt 2>/dev/null                                                                                                                                         
                                                                                                                                                                                           
       Here's how you can use llamafile to describe a jpg/png/gif image:                                                                                                                   
                                                                                                                                                                                           
             llamafile --temp 0 \                                                                                                                                                          
               --image lemurs.jpg \                                                                                                                                                        
               -m llava-v1.5-7b-Q4_K.gguf \                                                                                                                                                
               --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \                                                                                                                                   
               -e -p '### User: What do you see?\n### Assistant: ' \                                                                                                                       
               --no-display-prompt 2>/dev/null                                                                                                                                             
                                                                                                                                                                                           
       If  you  wanted  to  write a script to rename all your image files, you                                                                                                             
       could use the following command to generate a safe filename:                                                                                                                        
                                                                                                                                                                                           
             llamafile --temp 0 \                                                                                                                                                          
                 --image ~/Pictures/lemurs.jpg \                                                                                                                                           
                 -m llava-v1.5-7b-Q4_K.gguf \                                                                                                                                              
                 --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \                                                                                                                                 
                 --grammar 'root ::= [a-z]+ (" " [a-z]+)+' \                                                                                                                               
                 -e -p '### User: The image has...\n### Assistant: ' \                                                                                                                     
                 --no-display-prompt 2>/dev/null |                                                                                                                                         
               sed -e's/ /_/g' -e's/$/.jpg/'                                                                                                                                               
             three_baby_lemurs_on_the_back_of_an_adult_lemur.jpg                                                                                                                           
                                                                                                                                                                                           
       Here's an example of how to make an API request to the OpenAI API  com‐                                                                                                             
       patible  completions  endpoint  when  your  llamafile is running in the                                                                                                             
       background in --server mode.                                                                                                                                                        
                                                                                                                                                                                           
             curl -s http://localhost:8080/v1/chat/completions \                                                                                                                           
                  -H "Content-Type: application/json" -d '{                                                                                                                                
               "model": "gpt-3.5-turbo",                                                                                                                                                   
               "stream": true,                                                                                                                                                             
               "messages": [                                                                                                                                                               
                 {                                                                                                                                                                         
                   "role": "system",                                                                                                                                                       
                   "content": "You are a poetic assistant."                                                                                                                                
                 },                                                                                                                                                                        
                 {                                                                                                                                                                         
                   "role": "user",                                                                                                                                                         
                   "content": "Compose a poem that explains FORTRAN."                                                                                                                      
                 }                                                                                                                                                                         
               ]                                                                                                                                                                           
             }' | python3 -c '                                                                                                                                                             
             import json                                                                                                                                                                   
             import sys                                                                                                                                                                    
             json.dump(json.load(sys.stdin), sys.stdout, indent=2)                                                                                                                         
             print()                                                                                                                                                                       
                                                                                                                                                                                           
PROTIP                                                                                                                                                                                     
       The -ngl 35 flag needs to be passed in order to use GPUs made by NVIDIA                                                                                                             
       and AMD.  It's not enabled by default since it sometimes  needs  to  be                                                                                                             
       tuned  based on the system hardware and model architecture, in order to                                                                                                             
       achieve optimal performance, and avoid compromising a shared display.                                                                                                               
                                                                                                                                                                                           
SEE ALSO                                                                                                                                                                                   
       llamafile-quantize(1),   llamafile-perplexity(1),    llava-quantize(1),                                                                                                             
       zipalign(1), unzip(1)                                                                                                                                                               
                                                                                                                                                                                           
Mozilla Ocho                   October 12, 2024                   LLAMAFILE(1)                                                                                                             
PS C:\Python\bot-llm> 