version: "3.8"

services:
  llm-service:
    build: .
    ports:
      - "8000:8000"
    volumes:
      # Mount the models directory to persist model files
      - ./src/models:/app/src/models
      # Mount the llamafile directory
      - ./src/llamafile:/app/src/llamafile
      # Mount temp directory for WSL compatibility
      - /tmp:/tmp/wsltmp
    environment:
      - PYTHONUNBUFFERED=1
      - TMPDIR=/tmp/wsltmp
    # Add capabilities needed for WSL
    cap_add:
      - SYS_ADMIN
    security_opt:
      - seccomp:unconfined
    # Limit memory usage
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    # Use host network for better WSL compatibility
    network_mode: "host"
    # Ensure container restarts on failure
    restart: unless-stopped
